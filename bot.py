import os
import requests
import time
from datetime import datetime
import re
import json
import threading
from http.server import HTTPServer, SimpleHTTPRequestHandler
from bs4 import BeautifulSoup
import random

TELEGRAM_TOKEN = "7908433957:AAEyetZTWACBNn6t-wHPQwB89p1PtkQEvfg"
CHANNEL_ID = "@fiveleaguesua"
ADMIN_CHAT_ID = "YOUR_ADMIN_CHAT_ID"  # –¢–≤—ñ–π chat_id –¥–ª—è –ø–æ–≥–æ–¥–∂–µ–Ω–Ω—è

CLUBS = {
    'manchester united': 'üî¥ –ú–∞–Ω—á–µ—Å—Ç–µ—Ä –Æ–Ω–∞–π—Ç–µ–¥',
    'manchester city': 'üîµ –ú–∞–Ω—á–µ—Å—Ç–µ—Ä –°—ñ—Ç—ñ', 
    'liverpool': 'üî¥ –õ—ñ–≤–µ—Ä–ø—É–ª—å',
    'chelsea': 'üîµ –ß–µ–ª—Å—ñ',
    'arsenal': 'üî¥ –ê—Ä—Å–µ–Ω–∞–ª',
    'tottenham': '‚ö™ –¢–æ—Ç—Ç–µ–Ω–≥–µ–º',
    'real madrid': '‚ö™ –†–µ–∞–ª –ú–∞–¥—Ä–∏–¥',
    'barcelona': 'üîµ –ë–∞—Ä—Å–µ–ª–æ–Ω–∞',
    'atletico madrid': 'üî¥ –ê—Ç–ª–µ—Ç—ñ–∫–æ –ú–∞–¥—Ä–∏–¥',
    'juventus': '‚ö´ –Æ–≤–µ–Ω—Ç—É—Å',
    'milan': 'üî¥ –ú—ñ–ª–∞–Ω',
    'inter milan': 'üîµ –Ü–Ω—Ç–µ—Ä',
    'bayern munich': 'üî¥ –ë–∞–≤–∞—Ä—ñ—è',
    'psg': 'üîµ –ü–°–ñ',
    'borussia dortmund': 'üü° –ë–æ—Ä—É—Å—ñ—è –î–æ—Ä—Ç–º—É–Ω–¥'
}

processed_articles = set()
pending_posts = {}

def get_user_agent():
    """–í–∏–ø–∞–¥–∫–æ–≤–∏–π User-Agent"""
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    ]
    return random.choice(agents)

def translate_text(text):
    """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥"""
    try:
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if len(text) > 400:
            text = text[:400]
            
        # –°–ø—Ä–æ–±—É—î–º–æ –∫—ñ–ª—å–∫–∞ —Å–µ—Ä–≤—ñ—Å—ñ–≤
        services = [
            "https://api.mymemory.translated.net/get",
            "https://translate.googleapis.com/translate_a/single"
        ]
        
        for service_url in services:
            try:
                if "mymemory" in service_url:
                    params = {'q': text, 'langpair': 'en|uk'}
                    response = requests.get(service_url, params=params, timeout=10)
                    data = response.json()
                    
                    if 'responseData' in data:
                        translated = data['responseData']['translatedText']
                        if "MYMEMORY WARNING" not in translated.upper():
                            return clean_translation(translated)
                            
                elif "googleapis" in service_url:
                    params = {
                        'client': 'gtx',
                        'sl': 'en',
                        'tl': 'uk',
                        'dt': 't',
                        'q': text
                    }
                    response = requests.get(service_url, params=params, timeout=10)
                    result = response.json()
                    if result and len(result) > 0 and len(result[0]) > 0:
                        translated = ''.join([x[0] for x in result[0] if x[0]])
                        return clean_translation(translated)
                        
            except:
                continue
                
        return text
    except:
        return text

def clean_translation(text):
    """–û—á–∏—â–µ–Ω–Ω—è –ø–µ—Ä–µ–∫–ª–∞–¥—É"""
    # –ó–∞–º—ñ–Ω–∏ –¥–ª—è –∫—Ä–∞—â–æ—ó —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó
    replacements = {
        '—ñ–Ω—Ç–µ—Ä–Ω–∞—Ü—ñ–æ–Ω–∞–ª–µ': '–º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–π',
        '—Ä–µ–∞–ª –º–∞–¥—Ä–∏–¥': '–†–µ–∞–ª –ú–∞–¥—Ä–∏–¥',
        '–±–∞—Ä—Å–µ–ª–æ–Ω–∞': '–ë–∞—Ä—Å–µ–ª–æ–Ω–∞',
        '–º–∞–Ω—á–µ—Å—Ç–µ—Ä —é–Ω–∞–π—Ç–µ–¥': '–ú–∞–Ω—á–µ—Å—Ç–µ—Ä –Æ–Ω–∞–π—Ç–µ–¥',
        '–ª—ñ–≤–µ—Ä–ø—É–ª—å': '–õ—ñ–≤–µ—Ä–ø—É–ª—å',
        '—á–µ–ª—Å—ñ': '–ß–µ–ª—Å—ñ',
        '–∞—Ä—Å–µ–Ω–∞–ª': '–ê—Ä—Å–µ–Ω–∞–ª',
        '–±–∞–≤–∞—Ä—ñ—è –º—é–Ω—Ö–µ–Ω': '–ë–∞–≤–∞—Ä—ñ—è',
        '–ø—Å –∂–µ—Ä–º–µ–Ω': '–ü–°–ñ'
    }
    
    text_lower = text.lower()
    for eng, ua in replacements.items():
        text = re.sub(re.escape(eng), ua, text, flags=re.IGNORECASE)
    
    return text

def get_club_name(text):
    """–û—Ç—Ä–∏–º—É—î–º–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–µ —ñ–º'—è –∫–ª—É–±—É"""
    text_lower = text.lower()
    for eng_name, ua_name in CLUBS.items():
        if eng_name in text_lower:
            return ua_name
    return None

def is_quality_news(title, description):
    """–®–Ü-—Ä–µ–¥–∞–∫—Ç–æ—Ä: —á–∏ –≤–∞—Ä—Ç–∞ –Ω–æ–≤–∏–Ω–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó?"""
    text = (title + " " + description).lower()
    
    # –û–±–æ–≤'—è–∑–∫–æ–≤—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ —è–∫—ñ—Å–Ω–æ—ó –Ω–æ–≤–∏–Ω–∏
    has_player_name = bool(re.search(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', title + " " + description))
    has_money = bool(re.search(r'(‚Ç¨|¬£|\$)\d+|million|billion', text))
    has_concrete_info = any(word in text for word in ['signed', 'agreed', 'confirmed', 'official', 'announced'])
    
    # –í—ñ–¥–∫–∏–¥–∞—î–º–æ –ø–æ–≥–∞–Ω—ñ –Ω–æ–≤–∏–Ω–∏
    bad_indicators = ['rumour', 'reportedly', 'could', 'might', 'possible', 'potential', 'linked with']
    has_bad_indicators = any(indicator in text for indicator in bad_indicators)
    
    # –¶—ñ–∫–∞–≤—ñ —Ç–µ–º–∏
    interesting_topics = ['transfer', 'signing', 'injury', 'sacked', 'appointed', 'record', 'goal', 'hat-trick']
    has_interesting_topic = any(topic in text for topic in interesting_topics)
    
    # –õ–æ–≥—ñ–∫–∞ –≤–∏–±–æ—Ä—É
    score = 0
    if has_player_name: score += 3
    if has_money: score += 2
    if has_concrete_info: score += 2
    if has_interesting_topic: score += 1
    if has_bad_indicators: score -= 2
    
    return score >= 4

def parse_bbc_sport():
    """–ü–∞—Ä—Å–∏–Ω–≥ BBC Sport Football"""
    try:
        url = "https://www.bbc.com/sport/football"
        headers = {'User-Agent': get_user_agent()}
        response = requests.get(url, headers=headers, timeout=15)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = []
        
        # –®—É–∫–∞—î–º–æ —Å—Ç–∞—Ç—Ç—ñ
        for article in soup.find_all(['article', 'div'], class_=re.compile(r'(story|article|item)')):
            try:
                title_elem = article.find(['h1', 'h2', 'h3'], class_=re.compile(r'(title|headline)'))
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                description = ""
                
                # –®—É–∫–∞—î–º–æ –æ–ø–∏—Å
                desc_elem = article.find('p')
                if desc_elem:
                    description = desc_elem.get_text(strip=True)
                
                if len(title) > 20 and is_quality_news(title, description):
                    article_hash = hash(title)
                    if article_hash not in processed_articles:
                        articles.append({
                            'title': title,
                            'description': description,
                            'source': 'BBC Sport',
                            'hash': article_hash
                        })
                        
            except:
                continue
                
        return articles[:5]  # –¢–æ–ø-5
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É BBC: {e}")
        return []

def parse_sky_sports():
    """–ü–∞—Ä—Å–∏–Ω–≥ Sky Sports"""
    try:
        url = "https://www.skysports.com/football/news"
        headers = {'User-Agent': get_user_agent()}
        response = requests.get(url, headers=headers, timeout=15)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = []
        
        for article in soup.find_all(['div', 'article'], class_=re.compile(r'(news-list|story)')):
            try:
                title_elem = article.find(['h1', 'h2', 'h3', 'h4'])
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                description = ""
                
                desc_elem = article.find('p')
                if desc_elem:
                    description = desc_elem.get_text(strip=True)
                
                if len(title) > 20 and is_quality_news(title, description):
                    article_hash = hash(title)
                    if article_hash not in processed_articles:
                        articles.append({
                            'title': title,
                            'description': description,
                            'source': 'Sky Sports',
                            'hash': article_hash
                        })
                        
            except:
                continue
                
        return articles[:5]
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É Sky Sports: {e}")
        return []

def parse_marca():
    """–ü–∞—Ä—Å–∏–Ω–≥ Marca"""
    try:
        url = "https://www.marca.com/en/football.html"
        headers = {'User-Agent': get_user_agent()}
        response = requests.get(url, headers=headers, timeout=15)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = []
        
        for article in soup.find_all(['article', 'div'], class_=re.compile(r'(article|story|news)')):
            try:
                title_elem = article.find(['h1', 'h2', 'h3'])
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                description = ""
                
                desc_elem = article.find('p')
                if desc_elem:
                    description = desc_elem.get_text(strip=True)
                
                if len(title) > 20 and is_quality_news(title, description):
                    article_hash = hash(title)
                    if article_hash not in processed_articles:
                        articles.append({
                            'title': title,
                            'description': description,
                            'source': 'Marca',
                            'hash': article_hash
                        })
                        
            except:
                continue
                
        return articles[:5]
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É Marca: {e}")
        return []

def get_football_news():
    """–ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏ –∑ —É—Å—ñ—Ö –¥–∂–µ—Ä–µ–ª"""
    all_articles = []
    
    print("üì° BBC Sport...")
    all_articles.extend(parse_bbc_sport())
    
    time.sleep(2)  # –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
    
    print("üì° Sky Sports...")
    all_articles.extend(parse_sky_sports())
    
    time.sleep(2)
    
    print("üì° Marca...")
    all_articles.extend(parse_marca())
    
    # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —è–∫—ñ—Å—Ç—é
    if all_articles:
        # –í—ñ–¥–±–∏—Ä–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É
        best_article = max(all_articles, key=lambda x: len(x['title']) + len(x.get('description', '')))
        processed_articles.add(best_article['hash'])
        return [format_post(best_article['title'], best_article['description'], best_article['source'])]
    
    return []

def format_post(title, description, source="Football News"):
    """–§–æ—Ä–º—É—î–º–æ –µ–º–æ—Ü—ñ–π–Ω–∏–π –ø–æ—Å—Ç —É —Å—Ç–∏–ª—ñ '–°–ø–æ—Ä—Ç—Å'"""
    post_type = determine_post_type(title, description)

    emoji_map = {
        "transfer": "üí∞",
        "injury": "üè•", 
        "manager": "üëî",
        "performance": "‚öΩ",
        "general": "üì∞"
    }
    emoji = emoji_map.get(post_type, "üì∞")

    # –ü–µ—Ä–µ–∫–ª–∞–¥
    title_ua = translate_text(title)
    desc_ua = translate_text(description) if description else ""

    # –í–∏–¥—ñ–ª–µ–Ω–Ω—è –∫–ª—É–±—É
    club = get_club_name(title + " " + description)
    club_str = f"{club} " if club else ""

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    headline = f"{emoji} {club_str}{title_ua.strip()}".strip()

    # –û—Å–Ω–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç ‚Äî –∫–æ—Ä–æ—Ç–∫–æ, –ø–æ –¥—ñ–ª—É, –µ–º–æ—Ü—ñ–π–Ω–æ
    short_desc = desc_ua.strip()
    if len(short_desc) > 300:
        short_desc = short_desc[:300] + "‚Ä¶"

    # –§—ñ–Ω–∞–ª–∫–∞
    punch = "üî• –£–≤—ñ–º–∫–Ω—É–≤ —Ä–µ–∂–∏–º –±–æ–º–±–∞—Ä–¥–∏—Ä–∞!" if post_type == "performance" else \
            "üí• –¶–µ –º–æ–∂–µ –∑–º—ñ–Ω–∏—Ç–∏ —Ö—ñ–¥ —Å–µ–∑–æ–Ω—É." if post_type == "transfer" else \
            "ü§ï –í—Ç—Ä–∞—Ç–∏–ª–∏ –∫–ª—é—á–æ–≤–æ–≥–æ –≥—Ä–∞–≤—Ü—è." if post_type == "injury" else \
            "üëÄ –°–ª—ñ–¥–∫—É—î–º–æ –∑–∞ —Ä–æ–∑–≤–∏—Ç–∫–æ–º."    

    # –§–æ—Ä–º—É—î–º–æ –ø–æ—Å—Ç
    post = f"<b>{headline}</b>\n\n"

"
    post += f"{short_desc}

"
    post += f"{punch}
"
    post += f"üì∞ –î–∂–µ—Ä–µ–ª–æ: <i>{source}</i>"

    return post

def send_message(text, chat_id=CHANNEL_ID, reply_markup=None):
    """–ù–∞–¥—Å–∏–ª–∞—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ Telegram"""
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    
    if reply_markup:
        data["reply_markup"] = json.dumps(reply_markup)
    
    try:
        response = requests.post(url, data=data, timeout=10)
        return response
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è: {e}")
        return None

def create_approval_keyboard():
    """–°—Ç–≤–æ—Ä—é—î–º–æ –∫–Ω–æ–ø–∫–∏ –¥–ª—è –ø–æ–≥–æ–¥–∂–µ–Ω–Ω—è"""
    return {
        "inline_keyboard": [
            [
                {"text": "‚úÖ –û–ø—É–±–ª—ñ–∫—É–≤–∞—Ç–∏", "callback_data": "approve"},
                {"text": "‚ùå –í—ñ–¥—Ö–∏–ª–∏—Ç–∏", "callback_data": "reject"}
            ]
        ]
    }

def handle_callback(update):
    """–û–±—Ä–æ–±–∫–∞ –Ω–∞—Ç–∏—Å–∫–∞–Ω–Ω—è –∫–Ω–æ–ø–æ–∫"""
    try:
        callback_data = update.get('callback_data')
        message_id = update.get('message', {}).get('message_id')
        
        if callback_data == 'approve' and message_id in pending_posts:
            # –ü—É–±–ª—ñ–∫—É—î–º–æ –ø–æ—Å—Ç
            post_text = pending_posts[message_id]
            result = send_message(post_text, CHANNEL_ID)
            
            if result and result.status_code == 200:
                # –†–µ–¥–∞–≥—É—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è–º
                edit_message(message_id, "‚úÖ <b>–û–ü–£–ë–õ–Ü–ö–û–í–ê–ù–û!</b>")
                print("‚úÖ –ü–æ—Å—Ç –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–æ!")
            else:
                edit_message(message_id, "‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó")
                
            del pending_posts[message_id]
            
        elif callback_data == 'reject' and message_id in pending_posts:
            edit_message(message_id, "‚ùå <b>–í–Ü–î–•–ò–õ–ï–ù–û</b>")
            del pending_posts[message_id]
            print("‚ùå –ü–æ—Å—Ç –≤—ñ–¥—Ö–∏–ª–µ–Ω–æ")
            
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ callback: {e}")

def edit_message(message_id, new_text):
    """–†–µ–¥–∞–≥—É—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è"""
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/editMessageText"
    data = {
        "chat_id": ADMIN_CHAT_ID,
        "message_id": message_id,
        "text": new_text,
        "parse_mode": "HTML"
    }
    
    try:
        requests.post(url, data=data, timeout=10)
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ä–µ–¥–∞–≥—É–≤–∞–Ω–Ω—è: {e}")

def check_updates():
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è Telegram"""
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/getUpdates"
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            for update in data.get('result', []):
                if 'callback_query' in update:
                    handle_callback(update['callback_query'])
                    
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –æ–Ω–æ–≤–ª–µ–Ω—å: {e}")

def run_server():
    """HTTP —Å–µ—Ä–≤–µ—Ä –¥–ª—è Render"""
    port = int(os.environ.get("PORT", 5000))
    server = HTTPServer(('', port), SimpleHTTPRequestHandler)
    print(f"üåê –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω–æ –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    server.serve_forever()

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –±–æ—Ç–∞"""
    print("ü§ñ Five Leagues Bot v3.0 - –ó–∞–ø—É—Å–∫...")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–æ–∫–µ–Ω
    if ADMIN_CHAT_ID == "8142520596":
        print("‚ö†Ô∏è –£–í–ê–ì–ê: –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å ADMIN_CHAT_ID!")
    
    # –°—Ç–∞—Ä—Ç–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
    start_msg = "ü§ñ <b>Five Leagues Bot v3.0 –∑–∞–ø—É—â–µ–Ω–æ!</b> üá∫üá¶\n\n‚öΩ –ú–æ–Ω—ñ—Ç–æ—Ä–∏–º–æ BBC Sport, Sky Sports, Marca..."
    result = send_message(start_msg)
    
    if result and result.status_code == 200:
        print("‚úÖ –°—Ç–∞—Ä—Ç–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ")
    
    while True:
        try:
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"üïê {current_time} - –ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏...")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ callback'–∏
            check_updates()
            
            articles = get_football_news()
            
            if articles:
                best_article = articles[0]
                
                # –ù–∞–¥—Å–∏–ª–∞—î–º–æ –Ω–∞ –ø–æ–≥–æ–¥–∂–µ–Ω–Ω—è
                approval_msg = f"üìã <b>–ù–û–í–ò–ù–ê –ù–ê –ü–û–ì–û–î–ñ–ï–ù–ù–Ø:</b>\n\n{'-'*40}\n\n{best_article}\n\n{'-'*40}"
                
                keyboard = create_approval_keyboard()
                result = send_message(approval_msg, ADMIN_CHAT_ID, keyboard)
                
                if result and result.status_code == 200:
                    message_data = result.json()
                    message_id = message_data.get('result', {}).get('message_id')
                    if message_id:
                        pending_posts[message_id] = best_article
                        print("üì§ –ù–æ–≤–∏–Ω—É –Ω–∞–¥—ñ—Å–ª–∞–Ω–æ –Ω–∞ –ø–æ–≥–æ–¥–∂–µ–Ω–Ω—è")
                else:
                    print("‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è –Ω–æ–≤–∏–Ω–∏ –Ω–∞ –ø–æ–≥–æ–¥–∂–µ–Ω–Ω—è")
            else:
                print("üì∞ –ù–æ–≤–∏—Ö —è–∫—ñ—Å–Ω–∏—Ö –Ω–æ–≤–∏–Ω –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            
            print("‚è∞ –ù–∞—Å—Ç—É–ø–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ 30 —Ö–≤–∏–ª–∏–Ω...")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–∂–Ω—ñ 30 —Å–µ–∫—É–Ω–¥ –ø—Ä–æ—Ç—è–≥–æ–º 30 —Ö–≤–∏–ª–∏–Ω
            for _ in range(60):  # 60 * 30 —Å–µ–∫—É–Ω–¥ = 30 —Ö–≤–∏–ª–∏–Ω
                time.sleep(30)
                check_updates()
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Ü–∏–∫–ª—ñ: {e}")
            time.sleep(300)  # 5 —Ö–≤–∏–ª–∏–Ω –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ HTTP —Å–µ—Ä–≤–µ—Ä –≤ –æ–∫—Ä–µ–º–æ–º—É –ø–æ—Ç–æ—Ü—ñ
    threading.Thread(target=run_server, daemon=True).start()
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –±–æ—Ç–∞
    main()
