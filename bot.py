import os
import requests
import time
from datetime import datetime
import re
import json
import threading
from http.server import HTTPServer, SimpleHTTPRequestHandler
from bs4 import BeautifulSoup
import random

TELEGRAM_TOKEN = "7908433957:AAEyetZTWACBNn6t-wHPQwB89p1PtkQEvfg"
CHANNEL_ID = "@fiveleaguesua"
ADMIN_CHAT_ID = "YOUR_ADMIN_CHAT_ID"  # –¢–≤—ñ–π chat_id –¥–ª—è –ø–æ–≥–æ–¥–∂–µ–Ω–Ω—è

CLUBS = {
    'manchester united': 'üî¥ –ú–∞–Ω—á–µ—Å—Ç–µ—Ä –Æ–Ω–∞–π—Ç–µ–¥',
    'manchester city': 'üîµ –ú–∞–Ω—á–µ—Å—Ç–µ—Ä –°—ñ—Ç—ñ',
    'liverpool': 'üî¥ –õ—ñ–≤–µ—Ä–ø—É–ª—å',
    'chelsea': 'üîµ –ß–µ–ª—Å—ñ',
    'arsenal': 'üî¥ –ê—Ä—Å–µ–Ω–∞–ª',
    'tottenham': '‚ö™ –¢–æ—Ç—Ç–µ–Ω–≥–µ–º',
    'real madrid': '‚ö™ –†–µ–∞–ª –ú–∞–¥—Ä–∏–¥',
    'barcelona': 'üîµ –ë–∞—Ä—Å–µ–ª–æ–Ω–∞',
    'atletico madrid': 'üî¥ –ê—Ç–ª–µ—Ç—ñ–∫–æ –ú–∞–¥—Ä–∏–¥',
    'juventus': '‚ö´ –Æ–≤–µ–Ω—Ç—É—Å',
    'milan': 'üî¥ –ú—ñ–ª–∞–Ω',
    'inter milan': 'üîµ –Ü–Ω—Ç–µ—Ä',
    'bayern munich': 'üî¥ –ë–∞–≤–∞—Ä—ñ—è',
    'psg': 'üîµ –ü–°–ñ',
    'borussia dortmund': 'üü° –ë–æ—Ä—É—Å—ñ—è –î–æ—Ä—Ç–º—É–Ω–¥'
}

processed_articles = set()
pending_posts = {}

def get_user_agent():
    """–í–∏–ø–∞–¥–∫–æ–≤–∏–π User-Agent"""
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)',
        'Mozilla/5.0 (X11; Linux x86_64)'
    ]
    return random.choice(agents)

def translate_text(text):
    """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –ø–µ—Ä–µ–∫–ª–∞–¥"""
    try:
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        if len(text) > 400:
            text = text[:400]
        services = [
            "https://api.mymemory.translated.net/get",
            "https://translate.googleapis.com/translate_a/single"
        ]
        for service_url in services:
            try:
                if "mymemory" in service_url:
                    params = {'q': text, 'langpair': 'en|uk'}
                    response = requests.get(service_url, params=params, timeout=10)
                    data = response.json()
                    if 'responseData' in data:
                        translated = data['responseData']['translatedText']
                        if "MYMEMORY WARNING" not in translated.upper():
                            return clean_translation(translated)
                elif "googleapis" in service_url:
                    params = {
                        'client': 'gtx',
                        'sl': 'en',
                        'tl': 'uk',
                        'dt': 't',
                        'q': text
                    }
                    response = requests.get(service_url, params=params, timeout=10)
                    result = response.json()
                    if result and len(result) > 0 and len(result[0]) > 0:
                        translated = ''.join([x[0] for x in result[0] if x[0]])
                        return clean_translation(translated)
            except:
                continue
        return text
    except:
        return text

def clean_translation(text):
    """–û—á–∏—â–µ–Ω–Ω—è –ø–µ—Ä–µ–∫–ª–∞–¥—É"""
    replacements = {
        '—ñ–Ω—Ç–µ—Ä–Ω–∞—Ü—ñ–æ–Ω–∞–ª–µ': '–º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–π',
        '—Ä–µ–∞–ª –º–∞–¥—Ä–∏–¥': '–†–µ–∞–ª –ú–∞–¥—Ä–∏–¥',
        '–±–∞—Ä—Å–µ–ª–æ–Ω–∞': '–ë–∞—Ä—Å–µ–ª–æ–Ω–∞',
        '–º–∞–Ω—á–µ—Å—Ç–µ—Ä —é–Ω–∞–π—Ç–µ–¥': '–ú–∞–Ω—á–µ—Å—Ç–µ—Ä –Æ–Ω–∞–π—Ç–µ–¥',
        '–ª—ñ–≤–µ—Ä–ø—É–ª—å': '–õ—ñ–≤–µ—Ä–ø—É–ª—å',
        '—á–µ–ª—Å—ñ': '–ß–µ–ª—Å—ñ',
        '–∞—Ä—Å–µ–Ω–∞–ª': '–ê—Ä—Å–µ–Ω–∞–ª',
        '–±–∞–≤–∞—Ä—ñ—è –º—é–Ω—Ö–µ–Ω': '–ë–∞–≤–∞—Ä—ñ—è',
        '–ø—Å –∂–µ—Ä–º–µ–Ω': '–ü–°–ñ'
    }
    for eng, ua in replacements.items():
        text = re.sub(re.escape(eng), ua, text, flags=re.IGNORECASE)
    return text

def get_club_name(text):
    """–û—Ç—Ä–∏–º—É—î–º–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫–µ —ñ–º'—è –∫–ª—É–±—É"""
    text_lower = text.lower()
    for eng_name, ua_name in CLUBS.items():
        if eng_name in text_lower:
            return ua_name
    return None

def is_quality_news(title, description):
    """–ß–∏ –≤–∞—Ä—Ç–∞ –Ω–æ–≤–∏–Ω–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó?"""
    text = (title + " " + description).lower()
    has_player_name = bool(re.search(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', text))
    has_money = bool(re.search(r'(‚Ç¨|¬£|\$)\d+|million|billion', text))
    has_concrete_info = any(w in text for w in ['signed','agreed','confirmed','official','announced'])
    bad_indicators = ['rumour','reportedly','could','might','possible','potential','linked with']
    has_bad = any(ind in text for ind in bad_indicators)
    topics = ['transfer','signing','injury','sacked','appointed','record','goal','hat-trick']
    has_topic = any(t in text for t in topics)
    score = 0
    if has_player_name: score += 3
    if has_money: score += 2
    if has_concrete_info: score += 2
    if has_topic: score += 1
    if has_bad: score -= 2
    return score >= 4

def parse_bbc_sport():
    """–ü–∞—Ä—Å–∏–Ω–≥ BBC Sport"""
    try:
        url = "https://www.bbc.com/sport/football"
        headers = {'User-Agent': get_user_agent()}
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.content, 'html.parser')
        arts = []
        for art in soup.find_all(['article','div'], class_=re.compile(r'(story|article|item)')):
            try:
                t = art.find(['h1','h2','h3'], class_=re.compile(r'(title|headline)'))
                if not t: continue
                title = t.get_text(strip=True)
                desc = art.find('p').get_text(strip=True) if art.find('p') else ""
                if len(title)>20 and is_quality_news(title, desc):
                    h = hash(title)
                    if h not in processed_articles:
                        arts.append({'title':title,'description':desc,'source':'BBC Sport','hash':h})
            except:
                continue
        return arts[:5]
    except Exception as e:
        print(f"‚ùå BBC error: {e}")
        return []

def parse_sky_sports():
    """–ü–∞—Ä—Å–∏–Ω–≥ Sky Sports"""
    try:
        url = "https://www.skysports.com/football/news"
        headers = {'User-Agent': get_user_agent()}
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.content, 'html.parser')
        arts = []
        for art in soup.find_all(['div','article'], class_=re.compile(r'(news-list|story)')):
            try:
                t = art.find(['h1','h2','h3','h4'])
                if not t: continue
                title = t.get_text(strip=True)
                desc = art.find('p').get_text(strip=True) if art.find('p') else ""
                if len(title)>20 and is_quality_news(title, desc):
                    h = hash(title)
                    if h not in processed_articles:
                        arts.append({'title':title,'description':desc,'source':'Sky Sports','hash':h})
            except:
                continue
        return arts[:5]
    except Exception as e:
        print(f"‚ùå Sky error: {e}")
        return []

def parse_marca():
    """–ü–∞—Ä—Å–∏–Ω–≥ Marca"""
    try:
        url = "https://www.marca.com/en/football.html"
        headers = {'User-Agent': get_user_agent()}
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.content, 'html.parser')
        arts = []
        for art in soup.find_all(['article','div'], class_=re.compile(r'(article|story|news)')):
            try:
                t = art.find(['h1','h2','h3'])
                if not t: continue
                title = t.get_text(strip=True)
                desc = art.find('p').get_text(strip=True) if art.find('p') else ""
                if len(title)>20 and is_quality_news(title, desc):
                    h = hash(title)
                    if h not in processed_articles:
                        arts.append({'title':title,'description':desc,'source':'Marca','hash':h})
            except:
                continue
        return arts[:5]
    except Exception as e:
        print(f"‚ùå Marca error: {e}")
        return []

def get_football_news():
    """–ó–±–∏—Ä–∞—î–º–æ –Ω–æ–≤–∏–Ω–∏"""
    all_arts = []
    all_arts.extend(parse_bbc_sport())
    time.sleep(2)
    all_arts.extend(parse_sky_sports())
    time.sleep(2)
    all_arts.extend(parse_marca())
    if all_arts:
        best = max(all_arts, key=lambda x: len(x['title'])+len(x['description']))
        processed_articles.add(best['hash'])
        return [format_post(best['title'], best['description'], best['source'])]
    return []

def format_post(title, description, source="Football News"):
    """–§–æ—Ä–º—É—î–º–æ –µ–º–æ—Ü—ñ–π–Ω–∏–π –ø–æ—Å—Ç —É —Å—Ç–∏–ª—ñ '–°–ø–æ—Ä—Ç—Å'"""
    post_type = determine_post_type(title, description)

    em...
